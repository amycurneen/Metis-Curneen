{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Death_Row_Data.csv\", encoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of executions: 549\n"
     ]
    }
   ],
   "source": [
    "words = df['Last Statement']\n",
    "print(\"Number of executions: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of statements: 447\n"
     ]
    }
   ],
   "source": [
    "words = words.dropna()\n",
    "print(\"Number of statements: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extra symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Webscraping gave me none english characters. clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = [doc.replace(\"\\x99\",\"\") for doc in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = [doc.replace(\"\\x98\",\"\") for doc in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = [doc.replace(\"\\x93\",\"\") for doc in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = [doc.replace(\"\\x80\",\"\") for doc in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = [doc.replace(\"\\x9d\",\"\") for doc in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = [doc.replace(\"\\x9c\",\"\") for doc in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = [doc.replace(\"Ã¢\",\"\") for doc in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = [doc.replace(\"\\'\",\"\") for doc in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = [doc.replace(\"\\n\",\"\") for doc in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = [doc.replace(\"\\r\",\"\") for doc in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop \"No statement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_all_values(words,'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_all_values(words,'None.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_all_values(words,'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_all_values(words,'This offender declined to make a last statement.  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_all_values(words,'This offender declined to make a last statement.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_all_values(words,'No, I have no final statement. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_all_values(words,'No last statement.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of statements: 436\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of statements: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length of statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love you Israel. \n",
      " Bye, Im Ready. \n",
      " Profanity directed toward staff.  \n",
      " Santajaib Singh Ji.  \n",
      "Goodbye.\n",
      "Yes, I do.\n",
      "Ill see you.\n",
      "Peace.\n",
      "High Flight (aviation poem)\n",
      "Thanked his family.\n",
      "Im ready, Warden.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1: 2,\n",
       "         3: 6,\n",
       "         4: 3,\n",
       "         5: 3,\n",
       "         6: 2,\n",
       "         7: 4,\n",
       "         8: 5,\n",
       "         9: 2,\n",
       "         10: 3,\n",
       "         11: 4,\n",
       "         12: 3,\n",
       "         13: 4,\n",
       "         14: 5,\n",
       "         15: 2,\n",
       "         16: 4,\n",
       "         17: 8,\n",
       "         18: 3,\n",
       "         19: 4,\n",
       "         20: 6,\n",
       "         21: 3,\n",
       "         22: 4,\n",
       "         23: 2,\n",
       "         24: 6,\n",
       "         25: 6,\n",
       "         26: 1,\n",
       "         27: 5,\n",
       "         28: 6,\n",
       "         29: 4,\n",
       "         30: 2,\n",
       "         31: 3,\n",
       "         32: 8,\n",
       "         33: 4,\n",
       "         34: 2,\n",
       "         35: 7,\n",
       "         36: 2,\n",
       "         37: 1,\n",
       "         38: 1,\n",
       "         39: 2,\n",
       "         40: 4,\n",
       "         42: 3,\n",
       "         43: 4,\n",
       "         44: 2,\n",
       "         45: 7,\n",
       "         46: 3,\n",
       "         47: 3,\n",
       "         48: 4,\n",
       "         49: 2,\n",
       "         50: 2,\n",
       "         51: 2,\n",
       "         52: 2,\n",
       "         53: 2,\n",
       "         54: 1,\n",
       "         55: 1,\n",
       "         56: 5,\n",
       "         57: 2,\n",
       "         58: 3,\n",
       "         59: 1,\n",
       "         60: 1,\n",
       "         61: 2,\n",
       "         62: 3,\n",
       "         63: 2,\n",
       "         64: 2,\n",
       "         65: 4,\n",
       "         66: 2,\n",
       "         67: 5,\n",
       "         68: 1,\n",
       "         69: 6,\n",
       "         70: 1,\n",
       "         71: 4,\n",
       "         73: 2,\n",
       "         74: 5,\n",
       "         75: 4,\n",
       "         76: 3,\n",
       "         77: 5,\n",
       "         78: 5,\n",
       "         79: 3,\n",
       "         81: 1,\n",
       "         82: 3,\n",
       "         83: 2,\n",
       "         84: 3,\n",
       "         85: 1,\n",
       "         86: 3,\n",
       "         87: 2,\n",
       "         88: 3,\n",
       "         89: 1,\n",
       "         90: 3,\n",
       "         92: 1,\n",
       "         93: 3,\n",
       "         94: 2,\n",
       "         95: 1,\n",
       "         96: 1,\n",
       "         97: 2,\n",
       "         98: 1,\n",
       "         99: 1,\n",
       "         100: 3,\n",
       "         101: 2,\n",
       "         102: 3,\n",
       "         104: 1,\n",
       "         105: 2,\n",
       "         106: 1,\n",
       "         107: 3,\n",
       "         108: 1,\n",
       "         109: 1,\n",
       "         111: 2,\n",
       "         112: 3,\n",
       "         113: 1,\n",
       "         115: 1,\n",
       "         116: 3,\n",
       "         117: 2,\n",
       "         118: 3,\n",
       "         119: 1,\n",
       "         120: 1,\n",
       "         121: 2,\n",
       "         122: 2,\n",
       "         123: 3,\n",
       "         124: 2,\n",
       "         125: 2,\n",
       "         127: 4,\n",
       "         129: 2,\n",
       "         131: 1,\n",
       "         132: 1,\n",
       "         134: 3,\n",
       "         135: 3,\n",
       "         137: 1,\n",
       "         138: 3,\n",
       "         140: 2,\n",
       "         142: 1,\n",
       "         143: 2,\n",
       "         145: 1,\n",
       "         147: 2,\n",
       "         148: 1,\n",
       "         150: 1,\n",
       "         151: 2,\n",
       "         153: 2,\n",
       "         158: 3,\n",
       "         160: 1,\n",
       "         161: 1,\n",
       "         162: 1,\n",
       "         165: 1,\n",
       "         166: 1,\n",
       "         174: 2,\n",
       "         176: 1,\n",
       "         179: 1,\n",
       "         184: 1,\n",
       "         187: 1,\n",
       "         188: 1,\n",
       "         191: 2,\n",
       "         197: 1,\n",
       "         201: 1,\n",
       "         202: 1,\n",
       "         204: 1,\n",
       "         209: 1,\n",
       "         210: 1,\n",
       "         217: 1,\n",
       "         218: 1,\n",
       "         219: 1,\n",
       "         223: 1,\n",
       "         224: 1,\n",
       "         225: 2,\n",
       "         226: 3,\n",
       "         229: 1,\n",
       "         232: 1,\n",
       "         233: 1,\n",
       "         234: 1,\n",
       "         238: 1,\n",
       "         239: 1,\n",
       "         241: 1,\n",
       "         245: 1,\n",
       "         247: 1,\n",
       "         256: 1,\n",
       "         262: 1,\n",
       "         263: 1,\n",
       "         264: 1,\n",
       "         269: 2,\n",
       "         274: 1,\n",
       "         276: 1,\n",
       "         280: 1,\n",
       "         290: 1,\n",
       "         298: 1,\n",
       "         301: 1,\n",
       "         303: 1,\n",
       "         315: 1,\n",
       "         316: 1,\n",
       "         334: 1,\n",
       "         335: 1,\n",
       "         358: 2,\n",
       "         359: 1,\n",
       "         374: 1,\n",
       "         406: 1,\n",
       "         447: 1,\n",
       "         478: 1,\n",
       "         482: 1,\n",
       "         510: 1,\n",
       "         684: 1,\n",
       "         1268: 1})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking length of statements\n",
    "\n",
    "lengths = []\n",
    "for doc in words:\n",
    "    lengths.append(len(doc.split()))\n",
    "    if len(doc.split())<=4:\n",
    "        print(doc)\n",
    "lengths.sort()\n",
    "Counter(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of statements: 405\n"
     ]
    }
   ],
   "source": [
    "# create list of shorter than X words statements\n",
    "\n",
    "words_short = list(filter(lambda x: len(x.split()) < 240, words))\n",
    "print(\"Number of statements: {}\".format(len(words_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer for parsing/counting words\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=3, stop_words=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tfidf.fit_transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "vect = CountVectorizer(ngram_range=(1,3), min_df=3, stop_words=ENGLISH_STOP_WORDS,strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vect.fit_transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=num_topics, max_iter = 15, random_state=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "document_topics = lda.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (5, 1082)\n"
     ]
    }
   ],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "12            love          thy           thou          proceed warden\n",
      "profanity     family        unto          shall         proceed       \n",
      "staff         thank         god allah     lots          final         \n",
      "oh lord       forgive       theres        leadeth       blessing      \n",
      "oh            sorry         brothers sisterssin           warden jones  \n",
      "hands         im            return        committed     jones         \n",
      "amen          know          sisters       guilty        ready         \n",
      "spirit        say           allah         crime         dont say      \n",
      "lord          tell          brothers      paths         dont          \n",
      "goodbye       like          continue      head oil      warden        \n",
      "love mom      god           strong        enemies       say           \n",
      "love          want          tell          valley        yall          \n",
      "mom           hope          god           walk valley   love          \n",
      "prison        yall          kingdom       fear evil     love yall     \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda.components_,axis=1)[:,::-1]\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=range(num_topics),feature_names=feature_names,\n",
    "                           sorting=sorting,topics_per_chunk=6,n_words=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.array([range(num_topics)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205  Can you hear me, Chris?  The Lord is my Shepherd; I shall not want.  He makes me to lie down in green pastures; He leads me beside the still waters.  He restores my soul;  He leads me in the paths of righteousness for His names sake.  Yea, though I walk through the valley of the shadow of death, I will fear no evil; for Thou art with me;  Thy rod and Thy staff comfort me\n",
      "90 Collins family, I know your not going to get the closure you are looking for tonight. I wish you the best. I prayed for yall every day and every night. I have only the warmest wishes\n",
      "227 The Lord is my Shepherd, I shall not want. He maketh me lie down in green pastures; He leadeth me beside the still waters, He restoreth my soul. He leadeth me in the paths of righteousness for His names sake. Yea, though I walk through the valley of the shadow of death, I will fear no evil; for Thou art with me\n",
      "240  Yes sir, members of Mrs. Sanchezs family, I dont know who you are and other people present.  As I said, Im taking responsibility fro the death of your daughter in 1983.  Im deeply sorry for the loss of your loved one\n",
      "244  Yes, sir. Wheres Mr. Marinos mother? Did you get my letter? Just wanted to let you know, I sincerely meant everything I wrote. I am sorry for the pain\n"
     ]
    }
   ],
   "source": [
    "topic_4 = np.argsort(document_topics[:,2])\n",
    "\n",
    "for i in topic_4[:5]:\n",
    "    print(i,'.'.join(words[i].split(\".\")[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244     Well, I dont have anything to say.  I am just sorry about what I did to Mr. Peters.  Thats all\n",
      "178     Yes sir, I do.  To the victims family.  I hope it helps a little.  I do not know how, but I hope it helps\n",
      "68    Youre not about to witness an execution, you are about to witness a murder. I am strapped down for something Marcus Rhodes did. I never killed anybody, ever. I love you, Mom\n",
      "408    Thanked his family.\n",
      "147     Yes, for all of those that want this to happen, I hope that you get what you want and it makes you feel better and that it gives you some kind of relief.  I dont know what else to say.  For those that I have hurt, I hope after a while it gets better.  I love you, I love you\n",
      "334    Keep it brief here. Just want to say, uh, family, take care of yourselves. Uh, look at this as a learning experience. Everything happens for a reason\n",
      "219    The statement that I would like to make is, none of this should have happened and now that Im dying, there is nothing left to worry about. I know it was a mistake. I have no one to blame but myself. Its no big deal about choosing right from wrong\n",
      "96    Yes, Aint no way fo fo, I Love all yall.  \n",
      "404    I am the sinner of all sinners.  I was responsible for the 75 and 79 cases. My trial was not just; it was not fair; they lied against me. I love all of those on Death Row, and I will always hold them in my hands\n",
      "199     Yes sir.  Today I go home to the Lord.  But first, I have to say something.  I am real sorry\n"
     ]
    }
   ],
   "source": [
    "topic = np.argsort(document_topics[:,1])\n",
    "\n",
    "for i in topic[:10]:\n",
    "    print(i,'  ','.'.join(words[i].split(\".\")[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
